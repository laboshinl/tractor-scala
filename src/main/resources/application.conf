tractor.block-size = 1000000//67108864
akka.cluster.seed-nodes = [
//  "akka.tcp://ClusterSystem@172.16.1.91:2551",
  //"akka.tcp://ClusterSystem@172.16.1.111:2551",
 // "akka.tcp://ClusterSystem@172.16.1.93:2551",
 // "akka.tcp://ClusterSystem@172.16.1.94:2551"
 "akka.tcp://ClusterSystem@192.168.2.160:2551"
  ]

akka.actor.warn-about-java-serializer-usage = off

akka.remote.transport-failure-detector {
  heartbeat-interval = 30 s
  acceptable-heartbeat-pause = 30 s
}

akka.remote.netty.tcp {
    hostname = "127.0.0.1"
    port = 2551
    bind-hostname = "0.0.0.0"
    bind-port = 2551
}

akka.actor.default-dispatcher.fork-join-executor {
  parallelism-min = 16
  parallelism-max = 16
}

akka.cluster {
  min-nr-of-members = 2
  failure-detector {
    threshold = 12.0
    min-std-deviation = 10 s
    heartbeat-interval = 30 s
    acceptable-heartbeat-pause = 30 s //30
  }
}


akka.remote {
  maximum-payload-bytes = 70000000 bytes
  netty.tcp {
    message-frame-size =  70000000b
    send-buffer-size =  70000000b
    receive-buffer-size =  70000000b
    maximum-frame-size = 70000000b
  }
}


akka.loggers = ["akka.event.slf4j.Slf4jLogger"]

akka.actor {
  provider = "akka.cluster.ClusterActorRefProvider"
  deployment {
    /aggregator {
      router = consistent-hashing-pool
      nr-of-instances = 12
//      target {
//        nodes = ["akka.tcp://ClusterSystem@172.16.1.91:2551", "akka.tcp://ClusterSystem@172.16.1.92:2551", "akka.tcp://ClusterSystem@172.16.1.93:2551", "akka.tcp://ClusterSystem@172.16.1.94:2551"]
//      }
//      routees.paths = ["/user/aggregator"]
      virtual-nodes-factor = 8
      cluster {
        enabled = on
        max-nr-of-instances-per-node = 5
        allow-local-routees = on
        //use-role = compute
      }
    }
    /printer {
      router = broadcast-pool
      nr-of-insances = 4
//      target {
//        nodes = ["akka.tcp://ClusterSystem@172.16.1.91:2551", "akka.tcp://ClusterSystem@172.16.1.92:2551", "akka.tcp://ClusterSystem@172.16.1.93:2551", "akka.tcp://ClusterSystem@172.16.1.94:2551"]
//      }
      cluster {
        enabled = on
        max-nr-of-instances-per-node = 1
        allow-local-routees = on
        //use-role = compute
      }
    }
    /tracker {
//      target {
//        nodes = ["akka.tcp://ClusterSystem@172.16.1.91:2551", "akka.tcp://ClusterSystem@172.16.1.92:2551", "akka.tcp://ClusterSystem@172.16.1.93:2551", "akka.tcp://ClusterSystem@172.16.1.94:2551"]
//      }
      router = consistent-hashing-pool
      nr-of-instances = 10
//      routees.paths = ["/user/tracker"]
      virtual-nodes-factor = 8
      cluster {
        enabled = on
        max-nr-of-instances-per-node = 3
        allow-local-routees = on
        //use-role = compute
      }
    }
    /reducer {
//      target {
//        nodes = ["akka.tcp://ClusterSystem@172.16.1.91:2551", "akka.tcp://ClusterSystem@172.16.1.92:2551", "akka.tcp://ClusterSystem@172.16.1.93:2551", "akka.tcp://ClusterSystem@172.16.1.94:2551"]
//      }
      router = consistent-hashing-pool
      nr-of-instances = 12
//      routees.paths = ["/user/reducer"]
      virtual-nodes-factor = 8
      cluster {
        enabled = on
        max-nr-of-instances-per-node = 3
        allow-local-routees = on
        //use-role = compute
      }
    }
    /mapper {
//      target {
//        nodes = ["akka.tcp://ClusterSystem@172.16.1.91:2551", "akka.tcp://ClusterSystem@172.16.1.93:2551", "akka.tcp://ClusterSystem@172.16.1.94:2551"]
//      }
      router =  random-pool //round-robin-pool
      nr-of-instances = 100
//      routees.paths = ["/user/mapper"]
      virtual-nodes-factor = 8
//      resizer {
//        lower-bound = 8
//        upper-bound = 100
//        messages-per-resize = 5
//      }
      cluster {
        enabled = on
        max-nr-of-instances-per-node = 8
        allow-local-routees = on
        //use-role = *
      }
    }
  }
}

akka{
  actor{
    kryo  {
      # Possibles values for type are: graph or nograph
      # graph supports serialization of object graphs with shared nodes
      # and cyclic references, but this comes at the expense of a small
      # overhead nograph does not support object grpahs with shared nodes,
      # but is usually faster
      type = "nograph"

      # Possible values for idstrategy are:
      # default, explicit, incremental, automatic
      #
      # default - slowest and produces bigger serialized representation.
      # Contains fully-qualified class names (FQCNs) for each class. Note
      # that selecting this strategy does not work in version 0.3.2, but
      # is available from 0.3.3 onward.
      #
      # explicit - fast and produces compact serialized representation.
      # Requires that all classes that will be serialized are pre-registered
      # using the "mappings" and "classes" sections. To guarantee that both
      # sender and receiver use the same numeric ids for the same classes it
      # is advised to provide exactly the same entries in the "mappings"
      # section.
      #
      # incremental - fast and produces compact serialized representation.
      # Support optional pre-registering of classes using the "mappings"
      # and "classes" sections. If class is not pre-registered, it will be
      # registered dynamically by picking a next available id To guarantee
      # that both sender and receiver use the same numeric ids for the same
      # classes it is advised to pre-register them using at least the "classes" section.
      #
      # automatic -  use the pre-registered classes with fallback to FQCNs
      # Contains fully-qualified class names (FQCNs) for each non pre-registered
      # class in the "mappings" and "classes" sections. That this strategy was
      # added in version 0.4.1 and will not work with the previous versions

      idstrategy = "default"

      # Define a default queue builder, by default ConcurrentLinkedQueue is used.
      # Create your own queue builder by implementing the trait QueueBuilder,
      # useful for paranoid GC users that want to use JCtools MpmcArrayQueue for example.
      #
      # If you pass a bounded queue make sure its capacity is equal or greater than the
      # maximum concurrent remote dispatcher threads your application will ever have
      # running; failing to do this will have a negative performance impact:
      #
      # custom-queue-builder = "a.b.c.KryoQueueBuilder"

      # Define a default size for byte buffers used during serialization
      buffer-size = 4096

      # The serialization byte buffers are doubled as needed until they
      # exceed max-buffer-size and an exception is thrown. Can be -1
      # for no maximum.
      max-buffer-size = -1

      # If set, akka uses manifests to put a class name
      # of the top-level object into each message
      use-manifests = false

      # The transformations that have be done while serialization
      # Supported transformations: compression and encryption
      # accepted values(comma separated if multiple): off | lz4 | deflate | aes
      # Transformations occur in the order they are specified
      post-serialization-transformations = "lz4"

      # Settings for aes encryption, if included in transformations AES
      # algo mode, key and custom key class can be specified AES algo mode
      # defaults to 'AES/CBC/PKCS5Padding' and key to 'ThisIsASecretKey'.
      # If custom key class is provided, Kryo will use the class specified
      # by a fully qualified class name to get custom AES key. Such a
      # class should define the method 'kryoAESKey'. This key overrides 'key'.
      # If class doesn't contain 'kryoAESKey' method, specified key is used.
      # If this is not present, default key is used


      # Log implicitly registered classes. Useful, if you want to know all
      # classes which are serialized. You can then use this information in
      # the mappings and/or classes sections
      implicit-registration-logging = false

      # If enabled, Kryo logs a lot of information about serialization process.
      # Useful for debugging and lowl-level tweaking
      kryo-trace = false

      # If proviced, Kryo uses the class specified by a fully qualified
      # class name to perform a custom initialization of Kryo instances in
      # addition to what is done automatically based on the config file.
      #kryo-custom-serializer-init = "CustomKryoSerializerInitFQCN"

      # If enabled, allows Kryo to resolve subclasses of registered Types.
      #
      # This is primarily useful when idstrategy is set to "explicit". In this
      # case, all classes to be serialized must be explicitly registered. The
      # problem is that a large number of common Scala and Akka types (such as
      # Map and ActorRef) are actually traits that mask a large number of
      # specialized classes that deal with various situations and optimizations.
      # It isn't straightforward to register all of these, so you can instead
      # register a single supertype, with a serializer that can handle *all* of
      # the subclasses, and the subclasses get serialized with that.
      #
      # Use this with care: you should only rely on this when you are confident
      # that the superclass serializer covers all of the special cases properly.
      resolve-subclasses = false

      # Define mappings from a fully qualified class name to a numeric id.
      # Smaller ids lead to smaller sizes of serialized representations.
      #
      # This section is:
      # - mandatory for idstrategy="explicit"
      # - ignored   for idstrategy="default"
      # - optional  for incremental and automatic
      #
      # The smallest possible id should start at 20 (or even higher), because
      # ids below it are used by Kryo internally e.g. for built-in Java and
      # Scala types


      # Define a set of fully qualified class names for
      # classes to be used for serialization.
      # The ids for those classes will be assigned automatically,
      # but respecting the order of declaration in this section
      #
      # This section is ignored for idstrategy="default" and optional for
      # all other.

    }
  }
}

akka.actor.serializers {
  java = "akka.serialization.JavaSerializer"
  kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
}
akka.actor.serialization-bindings {
  "ru.laboshinl.tractor.TractorPacket" = kryo
  "ru.laboshinl.tractor.TractorPayload" = kryo
  "ru.laboshinl.tractor.WorkerMsg" = kryo
  "ru.laboshinl.tractor.TrackerMsg" = kryo
  //"ru.laboshinl.tractor.MapperMsg" = kryo
 // "ru.laboshinl.tractor.MapperMsg2" = kryo
 // "ru.laboshinl.tractor.AggregatorMsg" = kryo
  "java.util.UUID" = kryo
}
# Enable metrics extension in akka-cluster-metrics.
akka.extensions=["com.romix.akka.serialization.kryo.KryoSerializationExtension$"]
akka.cluster.metrics.enabled=off
# Settings for the ClusterShardingExtension
akka.persistence.journal.plugin="akka.persistence.journal.inmem"
akka.cluster.sharding {

  # The extension creates a top level actor with this name in top level system scope,
  # e.g. '/system/sharding'
  guardian-name = sharding

  # Specifies that entities runs on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  role = ""

  # When this is set to 'on' the active entity actors will automatically be restarted
  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
  # due to rebalance or crash.
  remember-entities = off

  # If the coordinator can't store state changes it will be stopped
  # and started again after this duration, with an exponential back-off
  # of up to 5 times this duration.
  coordinator-failure-backoff = 5 s

  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2 s

  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000

  # Timeout of the shard rebalancing process.
  handoff-timeout = 60 s

  # Time given to a region to acknowledge it's hosting a shard.
  shard-start-timeout = 10 s

  # If the shard is remembering entities and can't store state changes
  # will be stopped and then started again after this duration. Any messages
  # sent to an affected entity may be lost in this process.
  shard-failure-backoff = 10 s

  # If the shard is remembering entities and an entity stops itself without
  # using passivate. The entity will be restarted after this duration or when
  # the next message for it is received, which ever occurs first.
  entity-restart-backoff = 10 s

  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10 s

  # Absolute path to the journal plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default journal plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  journal-plugin-id = ""

  # Absolute path to the snapshot plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default snapshot plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  snapshot-plugin-id = ""

  # Parameter which determines how the coordinator will be store a state
  # valid values either "persistence" or "ddata"
  # The "ddata" mode is experimental, since it depends on the experimental
  # module akka-distributed-data-experimental.
  state-store-mode = "persistence"

  # The shard saves persistent snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  snapshot-after = 1000

  # Setting for the default shard allocation strategy
  least-shard-allocation-strategy {
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    rebalance-threshold = 10

    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }

  # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
  # works only for state-store-mode = "ddata"
  waiting-for-state-timeout = 5 s

  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
  # works only for state-store-mode = "ddata"
  updating-state-timeout = 5 s

  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
  # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
  # entity actors at a fix rate. The default strategy "all".
  entity-recovery-strategy = "all"

  # Default settings for the constant rate entity recovery strategy
  entity-recovery-constant-rate-strategy {
    # Sets the frequency at which a batch of entity actors is started.
    frequency = 100 ms
    # Sets the number of entity actors to be restart at a particular interval
    number-of-entities = 5
  }

  # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
  # The "role" of the singleton configuration is not used. The singleton role will
  # be the same as "akka.cluster.sharding.role".
  coordinator-singleton = ${akka.cluster.singleton}

  # The id of the dispatcher to use for ClusterSharding actors.
  # If not specified default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  # This dispatcher for the entity actors is defined by the user provided
  # Props, i.e. this dispatcher is not used for the entity actors.
  use-dispatcher = ""
}

